# -- Cluster ceph.conf override
configOverride: |
  [global]
  # Removed osd_pool_default_size = 1
  # Removed mon_warn_on_pool_no_redundancy = false
toolbox:
  enabled: true
  resources:
    limits:
      memory: "1Gi"
    requests:
      cpu: null
      memory: "128Mi"  
monitoring:
  enabled: true
  createPrometheusRules: true
cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v20.2.0
    allowUnsupported: false
  mon:
    count: 3
  mgr:
    count: 2
    allowMultiplePerNode: true
    modules:
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    prometheusEndpoint: http://rook-prometheus:9090
    prometheusEndpointSSLVerify: false
    port: 8080
    ssl: false
  network:
    provider: multus
    selectors:
      public: public-net
      cluster: cluster-net
  annotations:
    dashboard:
      omni-kube-service-exposer.sidero.dev/label: Ceph Dashboard
      omni-kube-service-exposer.sidero.dev/port: "50082"
      omni-kube-service-exposer.sidero.dev/prefix: cephdash
      omni-kube-service-exposer.sidero.dev/icon: H4sICGfoVmgAA2NlcGhfbG9nb19pY29uXzE3MDQwNi5zdmcAZVbtTmQ3DH2VaPr7hsR24qQCpJbfPMRo2GWQZrurgpZ9/J7jXDqoFZIJSa4/jo9PuH39+Zx+fbv89Xp3OL+9/fj95ub9/T2/a/7+9/ONlFJucOOQ3l+e3s53h26HdP7y8nx+W+ufL1/e//z+6+6waRZJm2VLvWd35S/VcUhfXy6Xu8NvX4uJPR3ub38c387p6e7w2Gp28dRa9qmnTbKJbzUrXI2q+Nutw2EVGJW+ZZkd502zDmc8lSS5jLG1LMWS5dIdd3vvCbELnJV8Xfc8xkPruRRJFV8kQ7JdkvKOliwjlmfE0QdcGNzmGaLr/gVXMhFfj9JyH2nZEj+MM1LEqSfUVluqeY6GJDV818F8Z08tVzXWwxvNB6rqw3ClNGPFGqGmoEjHjmJn81ycFjG2SVAqPsWHFUCJczV7Y/VwJ7mZ4G5FkDYJZVPZ8gRQ1RoXnr23owJpABCWFVTkBwgrsfU/uD/TsuuU4A1kr1bj25aWXadAe5QO1zL6g1TeZQFCkKKTBABAl7bWZ0ShGx9p2d0No4/IRB/Mc+tsPSABX6QAfxKtkRIWqdDFrGnZ5YIp+J5PpFh3ey1SVxxPCrzHbuN4Q72SAqAT4POO+9YVAJuiLaBF4Ksp8EVDzbFdpwFd7ZZGVjSKLUIM9Kjoo05675Jro0+d6IGwZW600sh5QAfjpLh0iUOJIRiES7lGbwYcCiEEqTS3SlzbRLHZNswd8nMcMTZsJZtKJ/kc2fm45IL6CkpXpGClnTLA7cFDqeShVUwHuMtw0uSICjACyy6aY0Jri3HHPoLYbtcpwnXOY3MWqiyvDdREEgtBnOy+oSi0i1XKIIhksRVS3XTEbI8jM1m8LERAeoxJYzJDd7tOsQ+WIwCH14xJz7gxP9/ArEhszt1+cp3C9Ynyow7HzKlBEZhvZMXpXEllDnnYD9eAL0oKIOpu/4WDA0M42FX4A7iFlAJhAkpNAWW2CeHgViWiuTWPPgTfm1zQMCLUMFXOBh4rOJXCLFKjxRgONvkEDlB0wo8GabmCMJMfhWwYFForzi3SuSPLYBYBYJZDNLYdfoZ40JOZgp8XVuwhLP8fPbS+d5LTwMkJwi6zwEDyaPZgqadCAjdqUV1yEIIv5DAIKjSUrMIrpYEtxbkaFwQOVWOqpBE7249SMpCiWWmgY50qZwAB26a73Se8gnoDOi3xje/2A0jFkEB31r592HWKd6FwluD+hAo6SWlOSS8hvFAKCENwerTIq8y07If4DIx7FqjfOq8pzPXU8kqr7fZD04at834iJchNQzNxGwAj6UiNZDMyBwCROSw8u5KgVGuUxvRQGiNHgHbFRNMVk/+A6VREgImhxrMJzeB8YZ6CR1QPNIGtEWiSc/RL4WPBltVdc/A33yOqVzQWwrnaiq5SGEZQPVjA128sQpChhdIDx1HzdL7o4GfQibQFnYKI8RrN9omIId3TF2OVjH3E+1M7bmLg5MTnCRPcOWekeJV9Hfb1uhOzs++vNf9vwOm+jv1j3/PdH9PteqaHm/tb/iN1/w8KodprcAkAAA==
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical      
cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 3
      deviceClass: ssd
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
  - name: ceph-blockpool-fast
    spec:
      failureDomain: host
      replicated:
        size: 3
      deviceClass: ssd
    storageClass:
      enabled: true
      name: ceph-block-fast
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
cephFileSystems:
  - name: ceph-filesystem
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
        deviceClass: ssd
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
          name: data0
          deviceClass: ssd
      metadataServer:
        activeCount: 1
        activeStandby: true
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
        deviceClass: ssd
      dataPool:
        failureDomain: host
        replicated:
          size: 3
        parameters:
          bulk: "true"
        deviceClass: ssd
      preservePoolsOnDelete: true
      gateway:
        port: 80
        instances: 1
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      parameters:
        region: us-west-1
    ingress:
      enabled: false